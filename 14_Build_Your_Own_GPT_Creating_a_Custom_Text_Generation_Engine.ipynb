{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twUL4J3zk7Zk"
   },
   "source": [
    "# Tiny LLM Story Generator — Training Notebook\n",
    "\n",
    "**Purpose:** This notebook trains a compact GPT-2 style language model to generate short children’s stories using the **TinyStories** dataset. It covers data loading, tokenization, model configuration, custom training, checkpointing, and sampling from saved checkpoints.\n",
    "\n",
    "## What this notebook does\n",
    "1. **Setup (Colab + Dependencies):** Mount Google Drive for persistent storage and import core libraries (`transformers`, `datasets`, `torch`, etc.).  \n",
    "2. **Data:** Load `roneneldan/TinyStories` via Hugging Face Datasets and perform lightweight preprocessing/tokenization suitable for small-context language modeling.  \n",
    "3. **Model:** Initialize a small GPT-2 configuration (tokenizer + `GPT2LMHeadModel`) tailored for fast prototyping on limited resources.  \n",
    "4. **Training Loop:** Train with `AdamW`, gradient clipping, and mini-batches using `DataLoader`/`IterableDataset`; track loss and save periodic checkpoints.  \n",
    "5. **Logging & Plots:** Record training history (e.g., loss) and visualize progression to validate convergence.  \n",
    "6. **Checkpointing:** Persist tokenizer/model to Drive for later reuse and reproducibility.  \n",
    "7. **Inference:** Load a chosen checkpoint and generate stories to qualitatively evaluate results.\n",
    "\n",
    "## Why TinyStories?\n",
    "TinyStories is a curated corpus of short, simple narratives designed for training and evaluating small language models. It enables rapid experiments while demonstrating end-to-end LM training and text generation.\n",
    "\n",
    "## Requirements\n",
    "- Python 3.x, PyTorch, Transformers, Datasets, TQDM, Matplotlib  \n",
    "- Sufficient GPU (e.g., Colab T4/A100) recommended\n",
    "\n",
    "## Reproducibility & Tips\n",
    "- Fix random seeds for consistent runs.  \n",
    "- Start with a small context length and batch size; scale up gradually.  \n",
    "- Monitor loss curves; stop early if overfitting.  \n",
    "- Keep checkpoints versioned (e.g., `tinygpt2_epochN`).\n",
    "\n",
    "> **Reference Dataset:** `roneneldan/TinyStories` (Hugging Face Datasets).  \n",
    "> **Author:** Ashish (Data Science Mentor) — YYYY-MM-DD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUFbywcYlKB_"
   },
   "source": [
    "### 1. Google Drive Mount\n",
    "\n",
    "Mounts Google Drive in Colab to access and save files directly from your Drive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UDGsTaALLb7d",
    "outputId": "277e9175-88c6-4b9e-8ea2-573f66649a90"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ie1y9C0llSOg"
   },
   "source": [
    "### 2. Library Installation and Data Loading\n",
    "\n",
    "- Installs the **`datasets`** library.  \n",
    "- Suppresses warning messages for cleaner output.  \n",
    "- Imports essential libraries for data handling, tokenization, visualization, and model building.  \n",
    "- Loads the **TinyStories** dataset in streaming mode for training.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "fe9bbba9fcb7464ebd924d6ab9aa2d99",
      "743e54310fa340e2abfde58c8af66e06",
      "f580d0b173d04431a3ca235b75340a7e",
      "7d505aaaeeec4c848219b90e3b4a68c3",
      "623a477b8230485b8cd65af53d2df3d8",
      "c7bb1340650f4369bbd08c9c061738b5",
      "174f2c9b0e624931a563d233c440162e",
      "1b4117eae41047d7b77bd08b07e25b58",
      "6e91d0bf5f114249bc0d3114101bb8e9",
      "6c527adf9bde4eb688328abf6b550cae",
      "92fd084dce72446ca8fa8af241286d36"
     ]
    },
    "id": "yS_ATZUmQYn5",
    "outputId": "db4dce27-06f8-4050-d887-d81086267e2f"
   },
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import re\n",
    "import torch\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrAASV-ZlcPq"
   },
   "source": [
    "### 3. TinyStoriesStreamDataset Class\n",
    "\n",
    "- Creates a **streaming PyTorch dataset** for TinyStories text.  \n",
    "- Steps performed for each story:\n",
    "  1. **Skip short samples:** Stories shorter than `min_length` are ignored.  \n",
    "  2. **Clean text:**  \n",
    "     - Removes extra spaces and unwanted characters.  \n",
    "     - Replaces fancy quotes with standard quotes.  \n",
    "  3. **Tokenize:** Converts text into token IDs using a GPT-2 tokenizer.  \n",
    "  4. **Prepare training inputs:**  \n",
    "     - `input_ids`: All tokens except the last one.  \n",
    "     - `labels`: All tokens except the first one (for next-token prediction).  \n",
    "     - `attention_mask`: Marks which tokens are real vs. padding.  \n",
    "\n",
    "\n",
    "\n",
    "#### Example\n",
    "    **Input text:**  \n",
    "    `\"  “The dog runs!” said Tom.  \"`  \n",
    "\n",
    "    **After cleaning:**  \n",
    "    `\"The dog runs!\" said Tom.`  \n",
    "\n",
    "    **Tokenization output (IDs):**  \n",
    "    `[50256, 464, 3290, 1101, 0, 616, 640, 13]`  \n",
    "\n",
    "    **Prepared for training:**  \n",
    "    | input_ids                | labels                    |\n",
    "    |--------------------------|---------------------------|\n",
    "    | [50256, 464, 3290, 1101] | [464, 3290, 1101, 0]      |\n",
    "\n",
    "    This way, the model learns to predict the **next token** at each position.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uEgDK6OPQdcH"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "class TinyStoriesStreamDataset(IterableDataset):\n",
    "    def __init__(self, dataset_stream, tokenizer, block_size=512, min_length=30):\n",
    "        self.dataset = dataset_stream\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "        self.min_length = min_length\n",
    "\n",
    "    def __iter__(self):\n",
    "        for sample in self.dataset:\n",
    "            text = sample[\"text\"].strip()\n",
    "            if len(text) < self.min_length:\n",
    "                continue\n",
    "\n",
    "            text = re.sub(r'\\s+', ' ', text)\n",
    "            text = re.sub(r'[“”]', '\"', text)\n",
    "            text = re.sub(r\"[‘’]\", \"'\", text)\n",
    "            text = re.sub(r'[^a-zA-Z0-9.,!?\\'\"\\s]', '', text)\n",
    "\n",
    "            tokenized = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                add_special_tokens=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.block_size,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            input_ids = tokenized[\"input_ids\"][0]\n",
    "            attention_mask = tokenized[\"attention_mask\"][0]\n",
    "\n",
    "            yield {\n",
    "                \"input_ids\": input_ids[:-1],\n",
    "                \"labels\": input_ids[1:],\n",
    "                \"attention_mask\": attention_mask[:-1]\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2Y10fFcltP9"
   },
   "source": [
    "### 4. Load Tokenizer, DataLoader, Model, and Optimizer Setup\n",
    "\n",
    "1. **Training size & batching**\n",
    "   - Define total samples and `batch_size`; compute `max_batches_per_epoch` for progress tracking.\n",
    "\n",
    "2. **Tokenizer**\n",
    "   - Load GPT-2 tokenizer and set the **pad token** to EOS for consistent padding.\n",
    "\n",
    "3. **Streaming dataset → DataLoader**\n",
    "   - Wrap `TinyStoriesStreamDataset` with a `DataLoader` to yield mini-batches for training.\n",
    "\n",
    "4. **Model configuration**\n",
    "   - Build a **small GPT-2**:\n",
    "     - `vocab_size = len(tokenizer)`\n",
    "     - Context length: `n_positions = n_ctx = 512`\n",
    "     - Model width: `n_embd = 256`\n",
    "     - Depth/heads: `n_layer = 4`, `n_head = 4`\n",
    "     - Use tokenizer’s `pad_token_id`\n",
    "\n",
    "5. **Device placement**\n",
    "   - Move model to **GPU** if available; enable **DataParallel** when multiple GPUs exist.\n",
    "\n",
    "6. **Optimizer**\n",
    "   - Initialize **AdamW** with learning rate `5e-5` for stable transformer training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "0caa7eaa73944e42bd199d0530a25797",
      "e62e8131f74e4600a2a4a4993a0f3bf1",
      "29e967736e4f476ca3873ef89e8a8626",
      "c22655210fbe421aa5f9b212411546c4",
      "c35c8b1d34ef49c4ad5f44250fa82808",
      "5aa38f79c3b6453b8c738fbb5e8c23f4",
      "4244367236114520b56b91770c4a7b26",
      "cbd5cce9da404c79b7c09b7767bc0bc9",
      "e053ba5a180441cfb091a244385e43d6",
      "8c1690d8d69749edaa37110b6804042d",
      "e41a19d357644670a78b1d93e38d722a",
      "a20eb492d68e432a98b1ae353cf38bd8",
      "16a25a7dabc94dddadb371b5efd42907",
      "195ca4ac85b44264b381563105963dfd",
      "bf1320779e144f96a2155d73824610be",
      "548a4ccef279489d9c5a2b9d2562313d",
      "8d52d99e2ace4b289a71df0954123e7e",
      "fbe39df589e74a3b9961863a348ee233",
      "5338ce04382d4c659cbb4e047a0a91f3",
      "d3b41389ae684eb2a45e4df41fe8ee2d",
      "14bf46b10426486d9f8604bea59009e4",
      "5a049d708ef44e61a3bda29e8cab0cdd",
      "01c04fa908e64ec983d137515422bbdd",
      "19ccd2d6c1b24a109019fe3bf1bf3035",
      "ab57452387d044f399e35b619091d550",
      "fe992dd8926046608b93754c6d320df4",
      "4f34383b0eda4b6ab71e4a592b0f7ac7",
      "65313f53b6c449f9b24c894e64230988",
      "ac958c6646274b69999ee05de1a6a51b",
      "c52333504cd344daa165096f72faeac9",
      "1f570cb6bfc74b82b93827e4560bf585",
      "3046fd8c7b554790a9bdb7ab923f960c",
      "3fbbe6a1c1774bf193b2758be307aaf4",
      "7d90356a86f446acaa6bb336b1d13b8a",
      "052e8684f6e14e71ad083a975f7b2306",
      "14822906ab1841b3875480a2b96dad7d",
      "4f4e4fe84b054ab8bf5a2ccc653484df",
      "cd5cc36f24464a179d3b2c3b743e4b05",
      "dd54aed286d440789ae299a7ba9e8dd6",
      "66a9e28b9c3144f2adbd19b78284c450",
      "046f6fda7cf64b9b992e9c8040f94d1e",
      "cfe17f73fc8a45fdbf28fe7b6c38c12d",
      "84f6c29395d64895b5a83eea899ad463",
      "f3509b2b5e694bf69231e97f07f7cbc8",
      "5ce2268ebcf54d37abb21a5f06877f89",
      "fb2de79fb6e240acafd657e321b5de62",
      "5937fd731ae54ddfbc276a90e90d24bf",
      "9863674d7fb646b1920d2ccacb4a5935",
      "fb0ad690a1f341a486fdf54bf156641b",
      "27d30915ddbf465197a801b3afa697ed",
      "53aa9c28d1a6454e8cb6fd03d9220821",
      "2c985735377a4437ab77c8f485f72ce1",
      "fd880b7a46434a23a92d223aa1025460",
      "dd97e905983546f3b1bc1d2df8eb8fb0",
      "0c3311365a64491dab7837334b3f2edf"
     ]
    },
    "id": "Q0vYQUpvQfk6",
    "outputId": "0e815e3a-3dfb-48aa-8f73-c8978952833c"
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "total_samples = 2119719\n",
    "batch_size = 52 # This will be overridden in the training loop\n",
    "max_batches_per_epoch = total_samples // batch_size # This will be overridden in the training loop\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "stream_dataset = TinyStoriesStreamDataset(dataset, tokenizer, block_size=128) # Pass reduced block_size\n",
    "train_loader = DataLoader(stream_dataset, batch_size=batch_size) # batch_size here doesn't matter due to streaming dataset\n",
    "\n",
    "\n",
    "config = GPT2Config(\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_positions=128, # Reduced n_positions\n",
    "    n_ctx=128, # Reduced n_ctx\n",
    "    n_embd=32, # Drastically reduced embedding dimension\n",
    "    n_layer=1, # Minimal number of layers\n",
    "    n_head=1, # Minimal number of attention heads\n",
    "    pad_token_id=tokenizer.pad_token_id)\n",
    "\n",
    "\n",
    "model = GPT2LMHeadModel(config)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Li16Hcuul7Tr"
   },
   "source": [
    "### 5. Training Loop, Checkpointing, and Sampling\n",
    "\n",
    "1. **Setup**\n",
    "   - Define a checkpoint folder on Google Drive.\n",
    "   - Set number of epochs and initialize a loss history list.\n",
    "   - Switch model to training mode.\n",
    "\n",
    "2. **Epoch training**\n",
    "   - For each epoch:\n",
    "     - Iterate over mini-batches up to `max_batches_per_epoch`.\n",
    "     - Move tensors to the selected device (CPU/GPU).\n",
    "     - Compute loss with labels for next-token prediction.\n",
    "     - Zero gradients → backpropagate → clip gradients (max norm = 1.0) → optimizer step.\n",
    "     - Accumulate batch losses.\n",
    "\n",
    "3. **Track progress**\n",
    "   - Compute and log **average loss** per epoch.\n",
    "   - Append the epoch’s average loss to `history`.\n",
    "\n",
    "4. **Checkpointing**\n",
    "   - Create an epoch-specific folder (e.g., `tinygpt2_epochN`).\n",
    "   - Save both the **model** and **tokenizer** to Drive after every epoch.\n",
    "\n",
    "5. **Qualitative check (sampling)**\n",
    "   - Temporarily switch to eval mode.\n",
    "   - Generate a short continuation from the prompt *“Once upon a time”*.\n",
    "   - Print the generated text to inspect model quality, then return to train mode.\n",
    "\n",
    "6. **Persist training history**\n",
    "   - Save the list of epoch losses to `training_history.json` on Drive for later plotting or review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411,
     "referenced_widgets": [
      "cfd11c3fd33a4fa7ae8ddd935e3cabb2",
      "ba10b09ad6cd47219519f47e6d10bbdf",
      "1ff50d8c7c5c47ad94a6f2cfaf7be9a1",
      "a76aee19a7874f02a0d0fdbb77429509",
      "51ee74867d93432bb30dc12563d70b40",
      "d9667d13541440eebd1a7fc9d1c541e3",
      "2fbe23b3860544a0b845ad9a9c4afb9f",
      "1e575995d16e41fab24766d0321e4079",
      "48f70d3ba307488aa09700325c5f8a17",
      "aded1433004646acbb06443cc9032b10",
      "f18348e2c9bc48b7a0079627d8718471",
      "5862afe2f97c4309a05ce9ec7d8c5ec5",
      "41fdda4a7f8341ef8c4ea1f6593e2597",
      "7090e80228164ee58ff0471ae82bd485",
      "b526ddb6b0de42efb13e6a6c5f463ec5",
      "24b9ac93f9854e98832d4d890e4b8808",
      "01310e16168e420bae99a7e6270cc062",
      "b1e9559707e448b6bbfa600e394e5f4b",
      "b3a1f348124f4f8894eca4b993a136c5",
      "e4907abb56c64d30ba27d34b5207136d",
      "c495e46ff78646fba562dc059e329ecc",
      "eacd7102b4cc4fd8b7b22ad9c8482d6a",
      "9f68d5b4b9eb4a99a57d980e8de068e3",
      "7a83c4fffb74413987309cbd188a4286",
      "9aa26c4d52c449bbb80bce7a1b6a4922",
      "21f19801d3724917aa7c2f38ed8c7ed8",
      "e019b79410ea4d9c9b49db2c14758977",
      "182e09d021eb4c7c9fc4c94d60aebb02",
      "1ae5f940c0d54f99996580359903c779",
      "27c478ba0c2848fc9133d111ea38d2fb",
      "8b5436b6340347688ef3f747277b0329",
      "da120d14239547da9f65401efc6d93dc",
      "6ec9b29c55974404a2be77266f39e681"
     ]
    },
    "id": "dgZF4U2nIkx_",
    "outputId": "3c2dbb0b-602a-44bb-ce05-2adecaddfdc8"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "# Define checkpoint directory\n",
    "checkpoint_dir = Path(\"/content/drive/MyDrive/TinyLLM/model/\")\n",
    "\n",
    "epochs = 10\n",
    "history = []\n",
    "\n",
    "model.train()\n",
    "\n",
    "total_samples = 2119719\n",
    "batch_size = 8 # Revert batch size to a reasonable value for training\n",
    "gradient_accumulation_steps = 8 # Revert accumulation steps to a reasonable value for training\n",
    "simulated_batch_size = batch_size * gradient_accumulation_steps\n",
    "max_batches_per_epoch = total_samples // simulated_batch_size\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for i, batch in enumerate(tqdm(train_loader, total=max_batches_per_epoch)):\n",
    "        if i >= max_batches_per_epoch:\n",
    "            break\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, labels=labels, attention_mask=attention_mask)\n",
    "        loss = outputs.loss / gradient_accumulation_steps  # Normalize loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        if (i + 1) % gradient_accumulation_steps == 0:\n",
    "            clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss += loss.item() * gradient_accumulation_steps # Scale loss back for reporting\n",
    "\n",
    "    # Perform a step after the loop if there are remaining gradients\n",
    "    if (i + 1) % gradient_accumulation_steps != 0:\n",
    "        clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    avg_loss = epoch_loss / max_batches_per_epoch\n",
    "    history.append(avg_loss)\n",
    "    print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Save model after every epoch\n",
    "    epoch_checkpoint = checkpoint_dir / f\"tinygpt2_epoch{epoch+1}\"\n",
    "    epoch_checkpoint.mkdir(parents=True, exist_ok=True)\n",
    "    model.save_pretrained(epoch_checkpoint)\n",
    "    tokenizer.save_pretrained(epoch_checkpoint)\n",
    "    print(f\"Model checkpoint saved at {epoch_checkpoint}\")\n",
    "\n",
    "    # Generate sample output\n",
    "    model.eval()\n",
    "    sample_input = tokenizer.encode(\"Once upon a time\", return_tensors=\"pt\").to(device)\n",
    "    generated_ids = model.generate(\n",
    "        sample_input,\n",
    "        max_length=50, # This should ideally also be <= the new block_size\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    print(f\"Sample Output:\\n{generated_text}\")\n",
    "    model.train()\n",
    "\n",
    "history_path = Path(\"/content/drive/MyDrive/TinyLLM/training_history.json\")\n",
    "with open(history_path, \"w\") as f:\n",
    "    json.dump(history, f)\n",
    "print(f\"\\nTraining history saved to {history_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ungs7_EHmElM"
   },
   "source": [
    "### 6. Resume Training from Checkpoint\n",
    "\n",
    "1. **Load checkpoint**\n",
    "   - Restore the model and tokenizer from `tinygpt2_epoch6`.\n",
    "\n",
    "2. **Configure training**\n",
    "   - Recreate optimizer, device placement (GPU if available), and batching parameters.\n",
    "\n",
    "3. **Continue epochs**\n",
    "   - Train from epoch 7 onward (up to the target `epochs`), repeating the standard loop:\n",
    "     - Forward pass → loss\n",
    "     - Zero grads → backward pass\n",
    "     - Gradient clipping (max norm = 1.0)\n",
    "     - Optimizer step\n",
    "\n",
    "4. **Checkpoint each epoch**\n",
    "   - Save model and tokenizer to `tinygpt2_epoch{N}` after every epoch.\n",
    "\n",
    "5. **Quick qualitative check**\n",
    "   - Switch to eval, generate a short continuation from “Once upon a time”, print sample, then return to train mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lSrC098mmqo8"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from transformers import GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "# Load model and tokenizer from checkpoint (epoch 6)\n",
    "checkpoint_path = Path(\"/content/drive/MyDrive/TinyLLM/model/tinygpt2_epoch6\")\n",
    "model = GPT2LMHeadModel.from_pretrained(checkpoint_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(checkpoint_path)\n",
    "\n",
    "total_samples = 2119719\n",
    "batch_size = 52\n",
    "max_batches_per_epoch = total_samples // batch_size\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training parameters\n",
    "checkpoint_dir = Path(\"/content/drive/MyDrive/TinyLLM/model/\")\n",
    "epochs = 12  # Continue up to epoch 10\n",
    "start_epoch = 6  # Start from epoch 6\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for i, batch in enumerate(tqdm(train_loader, total=max_batches_per_epoch)):\n",
    "        if i >= max_batches_per_epoch:\n",
    "            break\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, labels=labels, attention_mask=attention_mask)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / max_batches_per_epoch\n",
    "    print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Save model after each epoch\n",
    "    epoch_checkpoint = checkpoint_dir / f\"tinygpt2_epoch{epoch+1}\"\n",
    "    epoch_checkpoint.mkdir(parents=True, exist_ok=True)\n",
    "    model.save_pretrained(epoch_checkpoint)\n",
    "    tokenizer.save_pretrained(epoch_checkpoint)\n",
    "    print(f\"Model checkpoint saved at {epoch_checkpoint}\")\n",
    "\n",
    "    # Generate sample output\n",
    "    model.eval()\n",
    "    sample_input = tokenizer.encode(\"Once upon a time\", return_tensors=\"pt\").to(device)\n",
    "    generated_ids = model.generate(\n",
    "        sample_input,\n",
    "        max_length=50,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    print(f\"Sample Output:\\n{generated_text}\")\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7PWMzWJm2Oq"
   },
   "source": [
    "### 7. Generate Text from a Saved GPT-2 Checkpoint\n",
    "\n",
    "1. **Load model and tokenizer**\n",
    "   - Load tokenizer and model from a custom-trained checkpoint (`epoch_5`).\n",
    "\n",
    "2. **Define generation function**\n",
    "   - Encodes input text with attention masks.\n",
    "   - Uses `model.generate` to produce a continuation up to `max_len`.\n",
    "\n",
    "3. **Run examples**\n",
    "   - Generate short story snippets for several starting prompts (e.g., \"Once there was little boy\", \"Once there was a cute little\").\n",
    "\n",
    "- **Related Work:** A Kaggle-hosted version of this project is available here: [TinyStoryLLM by Ashish Jangra](https://www.kaggle.com/models/ashishjangra27/tinystoryllm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "th8m65_pmP55"
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "model_directory = \"epoch_5\"\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_directory)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_directory)\n",
    "\n",
    "\n",
    "def generate(input_text, max_len):\n",
    "\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "  inputs = tokenizer(\n",
    "      input_text,\n",
    "      return_tensors='pt',\n",
    "      padding=True,\n",
    "      return_attention_mask=True\n",
    "  )\n",
    "\n",
    "  output = model.generate(\n",
    "      input_ids=inputs['input_ids'],\n",
    "      attention_mask=inputs['attention_mask'],\n",
    "      max_length=max_len\n",
    "  )\n",
    "\n",
    "  generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "  return generated_text\n",
    "\n",
    "print(generate(\"Once there was little boy\",30))\n",
    "print(generate(\"Once there was little girl\",30))\n",
    "print(generate(\"Once there was a cute\",30))\n",
    "print(generate(\"Once there was a cute little\",30))\n",
    "print(generate(\"Once there was a handsome\",30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQnz2lvvmHar"
   },
   "source": [
    "### 8. Inference with Pretrained TinyStories Model\n",
    "\n",
    "1. **Load pretrained models**\n",
    "   - `AutoModelForCausalLM`: Loads the `roneneldan/TinyStories-3M` causal language model.  \n",
    "   - `AutoTokenizer`: Uses `EleutherAI/gpt-neo-125M` tokenizer for text processing.\n",
    "\n",
    "2. **Prepare input**\n",
    "   - Encode a simple prompt: `\"Once upon a time there was\"`.\n",
    "\n",
    "3. **Generate text**\n",
    "   - Use `model.generate` with `max_length=1000` to produce a story continuation.\n",
    "\n",
    "4. **Decode output**\n",
    "   - Convert token IDs back to readable text and print the generated story.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UKzLAnBcmHP9"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('roneneldan/TinyStories-3M')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "\n",
    "prompt = \"Once upon a time there was\"\n",
    "\n",
    "\n",
    "def generate(input_text, max_len):\n",
    "\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "  inputs = tokenizer(\n",
    "      input_text,\n",
    "      return_tensors='pt',\n",
    "      padding=True,\n",
    "      return_attention_mask=True\n",
    "  )\n",
    "\n",
    "  output = model.generate(\n",
    "      input_ids=inputs['input_ids'],\n",
    "      attention_mask=inputs['attention_mask'],\n",
    "      max_length=max_len\n",
    "  )\n",
    "\n",
    "  generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "  return generated_text\n",
    "\n",
    "  return output_text\n",
    "\n",
    "print(generate(\"Once there was little boy\",30))\n",
    "print(generate(\"Once there was little girl\",30))\n",
    "print(generate(\"Once there was a cute\",30))\n",
    "print(generate(\"Once there was a cute little\",30))\n",
    "print(generate(\"Once there was a handsome\",30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89ec3191"
   },
   "source": [
    "### Assignment: Code-Focused Inference\n",
    "\n",
    "Your task is to load a pre-trained GPT-2 model and configure it to answer *only* questions related to Python coding.\n",
    "\n",
    "1. **Load Model and Tokenizer:** Load a suitable pre-trained GPT-2 model and its corresponding tokenizer. You can use `transformers.AutoModelForCausalLM` and `transformers.AutoTokenizer`. A smaller model like `gpt2` or `gpt2-medium` might be sufficient.\n",
    "2. **Implement a Filtering Mechanism:** Before generating a response, check if the input prompt is related to Python coding. You can use simple keyword matching (e.g., \"Python\", \"code\", \"function\", \"class\", \"import\") or a more sophisticated approach using a text classification model (optional).\n",
    "3. **Generate Response:** If the prompt is deemed a Python coding question, generate a response using the loaded GPT-2 model.\n",
    "4. **Handle Non-Coding Questions:** If the prompt is not related to Python coding, return a predefined message indicating that the model can only answer coding questions.\n",
    "5. **Test:** Test your implementation with various prompts, including both Python coding questions and non-coding questions, to ensure the filtering mechanism works correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIQIO17ACPOa"
   },
   "source": [
    "# Assignment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d22ed4a3"
   },
   "source": [
    "#### 1. Load Model and Tokenizer\n",
    "\n",
    "We'll load the `gpt2` model and its tokenizer using the `transformers` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c7d2c1cc"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load a pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Set pad token id for generation\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Loaded model: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3940a3b6"
   },
   "source": [
    "#### 2. Implement Filtering Mechanism\n",
    "\n",
    "A simple keyword-based filtering mechanism is implemented to check if the input prompt is related to Python coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aa8f0502"
   },
   "outputs": [],
   "source": [
    "def is_python_coding_question(prompt):\n",
    "    \"\"\"Checks if the prompt is likely a Python coding question using keywords.\"\"\"\n",
    "    python_keywords = [\"python\", \"code\", \"function\", \"class\", \"import\", \"def \", \"lambda\", \"list\", \"dict\", \"tuple\", \"set\", \"string\", \"int\", \"float\", \"bool\", \"loop\", \"if\", \"else\", \"elif\", \"for\", \"while\", \"try\", \"except\", \"finally\", \"with\", \"open\", \"module\", \"package\", \"install\", \"pip\", \"environment\", \"variable\", \"syntax\", \"error\", \"debug\"]\n",
    "    prompt_lower = prompt.lower()\n",
    "    for keyword in python_keywords:\n",
    "        if keyword in prompt_lower:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6d25b713"
   },
   "source": [
    "#### 3. Generate Response and Handle Non-Coding Questions\n",
    "\n",
    "This function combines the filtering and generation steps. It generates a response only if the prompt is related to Python coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c3ea7341"
   },
   "outputs": [],
   "source": [
    "def generate_coding_response(prompt, max_length=100):\n",
    "    \"\"\"\n",
    "    Generates a response to a Python coding question using GPT-2.\n",
    "    Returns a predefined message for non-coding questions.\n",
    "    \"\"\"\n",
    "    if not is_python_coding_question(prompt):\n",
    "        return \"I can only answer questions related to Python coding.\"\n",
    "\n",
    "    # Encode the input prompt\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length - 20, # Leave some space for generation\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "\n",
    "    # Generate a response\n",
    "    output = model.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Decode and return the generated text\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Post-process: Find the prompt in the generated text and return the continuation\n",
    "    # This helps to remove the echoed prompt at the beginning of the generation\n",
    "    prompt_index = generated_text.lower().find(prompt.lower())\n",
    "    if prompt_index != -1:\n",
    "        generated_text = generated_text[prompt_index + len(prompt):].strip()\n",
    "\n",
    "    # Add a prefix to indicate it's a generated coding response\n",
    "    return \"Coding Response: \" + generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7f4afd39"
   },
   "source": [
    "#### 4. Test the Implementation\n",
    "\n",
    "Let's test with some example prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6691b55"
   },
   "outputs": [],
   "source": [
    "# Test cases\n",
    "coding_question_1 = \"How to define a function in Python?\"\n",
    "coding_question_2 = \"Explain list comprehensions in Python.\"\n",
    "non_coding_question_1 = \"What is the weather today?\"\n",
    "non_coding_question_2 = \"Tell me a story about a dragon.\"\n",
    "coding_question_3 = \"import pandas as pd\" # Test with just an import statement\n",
    "coding_question_4 = \"Write a Python code snippet for a for loop.\"\n",
    "\n",
    "\n",
    "print(f\"Prompt: {coding_question_1}\")\n",
    "print(generate_coding_response(coding_question_1))\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(f\"Prompt: {coding_question_2}\")\n",
    "print(generate_coding_response(coding_question_2))\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(f\"Prompt: {non_coding_question_1}\")\n",
    "print(generate_coding_response(non_coding_question_1))\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(f\"Prompt: {non_coding_question_2}\")\n",
    "print(generate_coding_response(non_coding_question_2))\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(f\"Prompt: {coding_question_3}\")\n",
    "print(generate_coding_response(coding_question_3))\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(f\"Prompt: {coding_question_4}\")\n",
    "print(generate_coding_response(coding_question_4))\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30a668c1"
   },
   "source": [
    "### Observations and Insights\n",
    "\n",
    "*   The keyword-based filtering is simple but effective for basic cases. It can be easily fooled by prompts containing keywords but not related to coding. A more robust approach would involve a dedicated text classification model trained on coding vs. non-coding text.\n",
    "*   The `gpt2` model, while relatively small, can generate plausible continuations for simple Python coding questions.\n",
    "*   The `model.generate` parameters like `max_length` and `no_repeat_ngram_size` are important for controlling the output quality and preventing repetitive text.\n",
    "*   Post-processing the generated text to remove the echoed prompt is necessary for cleaner output.\n",
    "*   For more complex coding questions, a larger, fine-tuned model or a model specifically trained on code (like CodeGPT or similar) would be required to provide accurate and helpful responses."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
